{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e6ffca2-1617-43b1-9724-2e134f825dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EEIC External r2b Parent\n",
    "\n",
    "##### This is the Parent notebook for EEIC-Checker-External-r2b scheme notebook\n",
    "\n",
    "- it contains all the functions that is needed for the external r2b notebook to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b65132a9-ca4b-4b8e-90da-8350c933787b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../BaseToCurated/General/BaseToCurated-Parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17133d3-2db3-4fb7-b495-4b436413fc11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, LongType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import coalesce\n",
    "import os\n",
    "from pyspark.sql.functions import to_date\n",
    "from functools import reduce\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d76551-c5da-48b1-a6ed-5e0b6dd1178c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LAKE_ROOT_SPARK = \"/mnt/datalake/\"\n",
    "external_scheme_path = \"/dbfs/mnt/datalake/Raw/PMDS/OfficialSensitive/External/ECO4/\"\n",
    "raw_path = \"Raw/PMDS/OfficialSensitive/\"\n",
    "base_path = \"Base/PMDS/OfficialSensitive/External/\"\n",
    "\n",
    "def is_valid_webportal_submission_path(path: str):\n",
    "    \"\"\"\n",
    "    Validates if the given path follows the expected directory structure and naming conventions for web portal submissions.\n",
    "\n",
    "    The expected structure is:\n",
    "    - The path should be nested at least 3 levels deep.\n",
    "    - The last three directories should represent a date in the format: YYYY/YYYYMM/YYYYMMDD.\n",
    "    - The year should be a 4-digit number within the range 1999 to 2049.\n",
    "    - The month and day should be valid numeric values.\n",
    "\n",
    "    Args:\n",
    "    path (str): The directory path to validate.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the path is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    dirs = path.split(\"/\")\n",
    "\n",
    "    # The directories should be nested at least 3 times\n",
    "    if len(dirs) < 3:\n",
    "        return False\n",
    "    \n",
    "    year, year_month, year_month_day = dirs[-3], dirs[-2], dirs[-1]\n",
    "\n",
    "    if len(year) != 4 or len(year_month) != 6 or len(year_month_day) != 8:\n",
    "        return False\n",
    "        \n",
    "    if not year.isnumeric() or not year_month.isnumeric() or not year_month_day.isnumeric():\n",
    "        return False\n",
    "\n",
    "    year_range = list(range(1999, 2050))\n",
    "    if int(year) not in year_range:\n",
    "        return False\n",
    "    \n",
    "    year_set = set((year, year_month[:4], year_month_day[:4]))\n",
    "\n",
    "    month_set = set((year_month[4:6], year_month_day[4:6]))\n",
    "\n",
    "    if len(year_set) != 1 or len(month_set) != 1:\n",
    "        return False\n",
    "    \n",
    "    day = year_month_day[6:8]\n",
    "    \n",
    "    if int(day) < 0 or int(day) > 31:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        time_object = datetime.strptime(dirs[-1], '%Y%m%d').date()\n",
    "    except(ValueError):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_path_of_latest_directory_from_webportal(path):\n",
    "    \"\"\"\n",
    "    Retrieves the path of the latest directory from the web portal submissions.\n",
    "\n",
    "    This function walks through the given directory path, validates each directory\n",
    "    based on the expected web portal submission structure, and identifies the latest\n",
    "    directory based on the date embedded in the directory name.\n",
    "\n",
    "    Args:\n",
    "    path (str): The root directory path to search for web portal submissions.\n",
    "\n",
    "    Returns:\n",
    "    str: The path of the latest directory that matches the expected structure and naming conventions.\n",
    "         Returns None if no valid directories are found.\n",
    "    \"\"\"\n",
    "    dir_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if is_valid_webportal_submission_path(root):\n",
    "            dir_list.append(root)\n",
    "\n",
    "\n",
    "    latest_date = datetime(1900, 1, 1).date()\n",
    "    latest_path = None\n",
    "\n",
    "    for dir in dir_list:\n",
    "        temp_date = datetime.strptime(dir.split('/')[-1], '%Y%m%d').date()\n",
    "        if temp_date > latest_date:\n",
    "            latest_date = temp_date\n",
    "            latest_path = dir\n",
    "    print(f'latest submission: {latest_date}')\n",
    "    print(f'file location path: {latest_path}')\n",
    "    return latest_path\n",
    "\n",
    "def get_path_of_latest_file_from_webportal_submissions(path):\n",
    "    \"\"\"\n",
    "    Retrieves the path of the latest CSV file from the web portal submissions.\n",
    "\n",
    "    This function identifies the latest directory based on the date embedded in the directory name,\n",
    "    then searches for the most recently modified CSV file within that directory.\n",
    "\n",
    "    Args:\n",
    "    path (str): The root directory path to search for web portal submissions.\n",
    "\n",
    "    Returns:\n",
    "    str: The path of the latest CSV file that matches the expected structure and naming conventions.\n",
    "         Raises FileNotFoundError if no files are found in the given path.\n",
    "    \"\"\"\n",
    "    latest_path = get_path_of_latest_directory_from_webportal(path)\n",
    "    if not os.listdir(latest_path) or latest_path is None:\n",
    "        raise FileNotFoundError(\"No files found in the given path.\")\n",
    "\n",
    "    last_modified = 0\n",
    "    latest_file_path = None\n",
    "\n",
    "    for file in os.listdir(latest_path):\n",
    "        file_path = latest_path + \"/\" + file\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        if not file_path.endswith(\".csv\"):\n",
    "            continue\n",
    "        temp_last_modified = os.path.getmtime(file_path)\n",
    "        if temp_last_modified > last_modified:\n",
    "            last_modified = temp_last_modified\n",
    "            latest_file_path = file_path\n",
    "    print(f'the file being processed: {os.path.basename(latest_file_path)}')\n",
    "    return latest_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9a9577-af42-45a7-be7e-3061b0f00b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, trim, when, expr\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"\n",
    "    Cleans the dataframe by performing the following operations on each column:\n",
    "    1. Replaces non-printable characters with None.\n",
    "    2. Trims whitespace and replaces specific values ('None', 'nan', '', 'NULL', 'N/A') with None.\n",
    "    3. Translates spaces to empty strings and replaces empty strings with None.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input Spark DataFrame to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned Spark DataFrame.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        df = df.withColumn(column, when(regexp_replace(col(column), '[^\\\\p{Print}]', '') == '', None).otherwise(col(column)))\n",
    "        df = df.withColumn(column, when(trim(col(column)).isin(['None', 'nan', '', 'NULL', 'N/A']), None).otherwise(trim(col(column))))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0096aa91-40d1-4322-8706-d382e3d78e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# return a dictionary of schemes, and their non-mandatory columns\n",
    "\n",
    "def get_scheme_schema_dictionary(onboarded_scheme_columns_dict: dict):\n",
    "    \"\"\"\n",
    "    Creates a dictionary where:\n",
    "        Keys -> represent the scheme name, must be unique (eco, gbis etc)\n",
    "        values -> represent the defined schema of the scheme, including the mandatory (uprn, date_of_install etc in their own wording) and non-mandatory columns (columns that are formatted into json in the measure details column).\n",
    "\n",
    "    Args:\n",
    "    df_onboarded_scheme_columns (DataFrame): A DataFrame containing the onboarded scheme columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with scheme names as keys and their corresponding schema as values.\n",
    "    \"\"\"\n",
    "\n",
    "    if onboarded_scheme_columns_dict == {}:\n",
    "        raise ValueError(\"The schema dictionary cannot be empty\")\n",
    "\n",
    "    onboarded_schemes_structtype_dict = {}\n",
    "\n",
    "    for scheme in onboarded_scheme_columns_dict:\n",
    "        columns = onboarded_scheme_columns_dict[scheme]\n",
    "        onboarded_schemes_structtype_dict[scheme] = StructType([StructField(element.strip(), StringType(), True) for element in columns])\n",
    "\n",
    "\n",
    "    return onboarded_schemes_structtype_dict\n",
    "\n",
    "\n",
    "\n",
    "def validate_eeic_reference_files(mandatory_column_mapping_path, onboarded_scheme_columns_path):\n",
    "    \"\"\"\n",
    "    Validates the consistency between the mandatory column mapping and onboarded scheme columns files.\n",
    "\n",
    "    This method asserts that the number of enrolled schemes within the 'mandatory_column_mapping.csv' \n",
    "    matches the number of schemes in the 'onboarded_scheme_columns.csv' file. It also checks if the \n",
    "    schemes listed in both files are identical and that every element (except the first) of each row \n",
    "    in df_mandatory_column_mapping exists somewhere in the row of df_onboarded_scheme_columns.\n",
    "\n",
    "    Args:\n",
    "    mandatory_column_mapping_path (str): The file path to the mandatory column mapping CSV.\n",
    "    onboarded_scheme_columns_path (str): The file path to the onboarded scheme columns CSV.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two Dictionaries:\n",
    "        - df_mandatory_column_mapping (DataFrame): Dataframe of the mandatory column mapping.\n",
    "        - df_onboarded_scheme_columns (Dictionary): Dictionary of the onboarded scheme columns.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the schemes in the mandatory column mapping and onboarded scheme columns CSV files do not match,\n",
    "                or if any mandatory column is not captured in the onboarded scheme columns.\n",
    "    \"\"\"\n",
    "    df_mandatory_column_mapping = pd.read_csv(mandatory_column_mapping_path)\n",
    "    df_onboarded_scheme_columns = pd.read_csv(onboarded_scheme_columns_path)\n",
    "\n",
    "    if 'scheme' not in df_mandatory_column_mapping:\n",
    "        raise KeyError(\"The mandatory column mapping csv file is missing the 'scheme' column\")\n",
    "\n",
    "    if 'scheme' not in df_onboarded_scheme_columns:\n",
    "        raise KeyError(\"The onboarded scheme columns csv file is missing the 'scheme' column\")\n",
    "\n",
    "    if not df_mandatory_column_mapping['scheme'].equals(df_onboarded_scheme_columns['scheme']):\n",
    "        raise ValueError(\"The schemes in the mandatory column mapping and onboarded scheme columns csv files do not match\")\n",
    "\n",
    "    onboarded_scheme_columns_dict = {}\n",
    "\n",
    "    for index, row in df_mandatory_column_mapping.iterrows():\n",
    "        scheme = df_onboarded_scheme_columns.iloc[index, 0]\n",
    "        mandatory_columns = row[1:].dropna().tolist()\n",
    "        onboarded_columns = df_onboarded_scheme_columns.iloc[index, 1:].dropna().tolist()[0]\n",
    "        onboarded_columns = ast.literal_eval(onboarded_columns)\n",
    "        onboarded_columns = [col.strip() for col in onboarded_columns]\n",
    "        for col in mandatory_columns:\n",
    "            if col not in onboarded_columns:\n",
    "                raise ValueError(f\"Mandatory column '{col}' from scheme '{row['scheme']}' is not captured in the onboarded scheme columns\")\n",
    "\n",
    "        onboarded_scheme_columns_dict[scheme] = onboarded_columns\n",
    "    return df_mandatory_column_mapping, onboarded_scheme_columns_dict\n",
    "\n",
    "def map_mandatory_columns(df, df_mandatory_column_mapping, scheme):\n",
    "    \"\"\"\n",
    "    Normalizes the names of the column schema to the recognized standard.\n",
    "\n",
    "    This function renames the columns of the given DataFrame based on a mapping provided\n",
    "    in the mandatory column mapping DataFrame. The mapping is specific to each scheme.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input Spark DataFrame whose columns need to be renamed.\n",
    "    df_mandatory_column_mapping (DataFrame): A DataFrame containing the mapping of old column names to new column names for each scheme.\n",
    "    scheme (str): The name of the scheme for which the column renaming should be applied.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns according to the specified scheme.\n",
    "    \"\"\"\n",
    "\n",
    "    # new column names\n",
    "    new_columns = df_mandatory_column_mapping.columns.values.tolist()[1:]\n",
    "\n",
    "    mappable_dict = df_mandatory_column_mapping.set_index('scheme').T.to_dict('list')\n",
    "\n",
    "    for current_scheme in mappable_dict:\n",
    "        if scheme == current_scheme:\n",
    "            old_columns = mappable_dict[scheme]\n",
    "            for old_column, new_column in zip(old_columns, new_columns):\n",
    "                if isinstance(old_column,str):\n",
    "                    df = df.withColumnRenamed(old_column, new_column)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# given an onboarded scheme and constructed schema, reads the submitted data\n",
    "def read_eeic_scheme(scheme_name, schema):\n",
    "    \"\"\"\n",
    "    Reads a CSV file for a given scheme name and schema from the latest submission within the web portal.\n",
    "\n",
    "    This function constructs the path to the latest CSV file for the specified scheme,\n",
    "    reads the file into a Spark DataFrame using the provided schema, and returns the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    scheme_name (str): The name of the scheme for which the CSV file should be read.\n",
    "    schema (StructType): The schema to be applied when reading the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A Spark DataFrame containing the data from the latest CSV file for the specified scheme.\n",
    "    \"\"\"\n",
    "    SCHEME_PATH = get_path_of_latest_file_from_webportal_submissions(LAKE_ROOT+\"Raw/PMDS/OfficialSensitive/External/\"+scheme_name).split(\"/datalake/\")[-1]\n",
    "    df = spark.read.option(\"header\", \"true\").schema(schema).option('encoding', 'utf8').option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"quote\", \"\\\"\").csv(LAKE_ROOT_SPARK+SCHEME_PATH)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_db_column(df, scheme_name):\n",
    "    \"\"\"\n",
    "    Adds a new column 'db_type' to the DataFrame with a constant value.\n",
    "\n",
    "    This function checks if the 'db_type' column already exists in the DataFrame.\n",
    "    If it does, it raises a ValueError. Otherwise, it adds a new column 'db_type'\n",
    "    with the value set to the provided scheme name.3\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input Spark DataFrame to which the 'db_type' column will be added.\n",
    "    scheme_name (str): The value to be assigned to the 'db_type' column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the added 'db_type' column.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the 'db_type' column already exists in the DataFrame.\n",
    "    \"\"\"\n",
    "    if \"db_type\" in df.columns:\n",
    "        raise ValueError(\"db_type column already exists, please rename column before continuing\")\n",
    "\n",
    "    df = df.withColumn(\"db_type\", f.lit(scheme_name))\n",
    "    return df\n",
    "                       \n",
    "def process_for_curated(df, columns):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame for the curated layer by building a 'measure_details' column.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Defines a list of columns to ignore.\n",
    "    2. Initializes the 'measure_details' column with None.\n",
    "    3. Builds the 'measure_details' column by concatenating non-ignored columns into a JSON string.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input Spark DataFrame to be processed.\n",
    "    scheme_name (str): The name of the scheme being processed.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The processed DataFrame with selected columns and the 'measure_details' column.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the 'measure_details' column already exists in the DataFrame.\n",
    "    \"\"\"\n",
    "    if \"measure_details\" in df.columns:\n",
    "        raise ValueError(\"measure_details column already exists, please rename column before continuing\")\n",
    "\n",
    "    minimal_ignore = [\"uprn\", \"date_of_install\", \"db_type\", \"measure_type\", \"postcode\"]\n",
    "\n",
    "    ignore = [] + minimal_ignore\n",
    "\n",
    "    # Build notes column\n",
    "    # columns = df.columns\n",
    "    df = df.withColumn(\"measure_details\", f.lit(None))\n",
    "    def build_measure_details(*args):\n",
    "        # Choose ignore set (only minimal if no UPRN)\n",
    "        this_ignore = [i.lower() for i in ignore]\n",
    "        d = {}\n",
    "        for i in range(len(columns)):\n",
    "            if columns[i].lower() not in this_ignore:\n",
    "                if isinstance(args[i], datetime):\n",
    "                    v = args[i].strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "                else:\n",
    "                    v = args[i]\n",
    "                d[columns[i]] = v\n",
    "        return json.dumps(d)\n",
    "\n",
    "    df = df.withColumn(\"measure_details\", f.udf(build_measure_details, t.StringType())(*columns))\n",
    "    display(df)\n",
    "\n",
    "    return df.select(\"uprn\", 'measure_type', \"date_of_install\", \"db_type\", 'measure_details')\n",
    "\n",
    "\n",
    "def get_measure_details_columns(onboarded_schemes_dict, df_mandatory_column_mapping, scheme_name):\n",
    "        filtered_row_by_scheme = df_mandatory_column_mapping[df_mandatory_column_mapping['scheme'] == scheme_name].iloc[0]\n",
    "\n",
    "        mandatory_to_remove = [x for x in filtered_row_by_scheme[1:].tolist() if isinstance(x, str)]\n",
    "        original_measure_details_columns = [column.name for column in onboarded_schemes_dict[scheme_name]]\n",
    "        measure_details_columns = list(set(original_measure_details_columns)- set(mandatory_to_remove))\n",
    "\n",
    "        return measure_details_columns\n",
    "\n",
    "def generic_pipeline(defined_schema_path=None):\n",
    "    \"\"\"\n",
    "    Executes a generic data processing pipeline for EEIC schemes.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Validates the reference files for mandatory column mapping and onboarded scheme columns.\n",
    "    2. Constructs a dictionary of onboarded schemes and their corresponding schemas.\n",
    "    3. Iterates over each scheme to read, clean, map mandatory columns, join to EEL standard, add db_type, \n",
    "       split valid and invalid UPRNs, save invalid UPRNs, process for curated layer, and append to a list.\n",
    "    4. Unions all processed DataFrames and saves the result to the curated layer.\n",
    "\n",
    "    Args:\n",
    "    defined_schema_path (str, optional): The file path to the directory containing the mandatory column mapping \n",
    "                                         and onboarded scheme columns CSV files. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    defined_schema = None\n",
    "    if defined_schema_path != None:\n",
    "\n",
    "        df_mandatory_column_mapping, onboarded_scheme_columns_dict = validate_eeic_reference_files(defined_schema_path + \"mandatory_column_mapping.csv\", defined_schema_path + \"onboarded_scheme_column_list.csv\")\n",
    "\n",
    "        onboarded_schemes_dict = get_scheme_schema_dictionary(onboarded_scheme_columns_dict)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for scheme_name in onboarded_schemes_dict.keys():\n",
    "        print(\"given a scheme name, read the scheme\", scheme_name)\n",
    "        df = read_eeic_scheme(scheme_name, onboarded_schemes_dict[scheme_name])\n",
    "\n",
    "        print(\"cleaning dataframe\")\n",
    "        df = clean_dataframe(df)\n",
    "        print(\"mapping mandatory columns to internal naming\")\n",
    "\n",
    "        # todo: If richard wants to remove subname, name, number etc, then\n",
    "        df = map_mandatory_columns(df, df_mandatory_column_mapping, scheme_name)\n",
    "\n",
    "\n",
    "        print(\"adding db_type\")\n",
    "        df = create_db_column(df, scheme_name)\n",
    "\n",
    "\n",
    "        print(\"processing for curated\")\n",
    "\n",
    "        measure_details_columns = get_measure_details_columns(onboarded_schemes_dict, df_mandatory_column_mapping, scheme_name)\n",
    "\n",
    "        df = process_for_curated(df, measure_details_columns)\n",
    "\n",
    "        df_list.append(df)\n",
    "        break\n",
    "\n",
    "    union_df = reduce(lambda df1, df2: df1.union(df2), df_list)\n",
    "    # union_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(LAKE_ROOT_SPARK + base_path + \"valid_measures\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone)_EEIC-Checker-External-r2b-Parent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}