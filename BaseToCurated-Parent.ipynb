{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb26bbe4-d031-46c2-8326-926a2b59d191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BaseToCurated-Parent - Installs & Imports\n",
    "\n",
    "#### Description:\n",
    "- Notebook containing the installs, imports and set up for base to curated notebooks\n",
    "- To be run as first action in base to curated notebooks\n",
    "\n",
    "#### Updated:\n",
    "- Created by Hannah 8/5/24\n",
    "- 13/5/24: Husain renamed the notebook, and tidied comments a little\n",
    "- 10/07/24: Kamel - added the imports for EEIC\n",
    "- 11/09/24: Junaid, Christos added imports for HUG2 ProjectInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabf6ce5-5bc7-4bea-8e8b-13ee63a95166",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ensure Libraries are Installed"
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Util/1-Installs-MASTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d62265ab-21b2-47be-a83d-cf365903a1b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Load Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9affed45-3f37-4ae4-82e0-f2b7eaa491f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports & Global Variables"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import inspect\n",
    "import unittest\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import expr, lit, row_number, when, from_json, udf, countDistinct, length, col, regexp_replace, upper, concat, row_number, count, trim, regexp_extract, format_string, isnan\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, IntegerType, DoubleType, NullType, LongType\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import Row\n",
    "\n",
    "try:\n",
    "    import pydms\n",
    "except ImportError:\n",
    "    # Add to the path so we can import in the next block\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), \"../../lib\"))\n",
    "try:\n",
    "    import pydms.file as dms_file\n",
    "    from pydms.util import exit_ as dms_exit\n",
    "    from pydms.util import get_text_widget, process_file_udf, get_secret, get_webportalDb_sqlDirectQuery\n",
    "    from pydms.data import get_column_lookups, deduplicate\n",
    "    from pydms.udf.cleaning import clean_values, load_regex_lookups\n",
    "    from pydms.udf.parsing import read_excel\n",
    "    from pydms.udf.util import get_container_entry_id\n",
    "except ImportError:\n",
    "    # These imports are for intelisense in pycharm environments, not expected to be used\n",
    "    import databricks.lib.pydms.file as dms_file\n",
    "    from databricks.lib.pydms.util import exit_ as dms_exit\n",
    "    from databricks.lib.pydms.util import get_text_widget, process_file_udf, get_secret, get_webportalDb_sqlDirectQuery\n",
    "    from databricks.lib.pydms.data import get_column_lookups, deduplicate\n",
    "    from databricks.lib.pydms.udf.cleaning import clean_values, load_regex_lookups\n",
    "    from databricks.lib.pydms.udf.parsing import read_excel\n",
    "    from databricks.lib.pydms.udf.util import get_container_entry_id\n",
    "from datetime import datetime\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.dbutils import DBUtils\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "try:\n",
    "    display\n",
    "except NameError:\n",
    "    def display(x):\n",
    "        print(x)\n",
    "spark.sparkContext.addPyFile(\"/dbfs/mnt/datalake/Raw/Databricks/NonSensitive/Libs/pydms-1-py3-none-any.whl\")\n",
    "\n",
    "LAKE_ROOT = \"/dbfs/mnt/datalake/\"\n",
    "LAKE_ROOT_SPARK = \"/mnt/datalake/\"\n",
    "\n",
    "# unit testing\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "from pyspark.errors import PySparkAssertionError\n",
    "from pyspark.errors.exceptions.base import PySparkTypeError\n",
    "\n",
    "\n",
    "print('Imports Loaded')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BaseToCurated-Parent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}